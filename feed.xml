<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jgcarrasco.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jgcarrasco.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-13T14:14:19+00:00</updated><id>https://jgcarrasco.github.io/feed.xml</id><title type="html">jgcarrasco</title><subtitle>Blog related to Deep Learning and Artificial Intelligence. </subtitle><entry><title type="html">Simulating games with neural networks</title><link href="https://jgcarrasco.github.io/blog/2025/neural-physics-engine/" rel="alternate" type="text/html" title="Simulating games with neural networks"/><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>https://jgcarrasco.github.io/blog/2025/neural-physics-engine</id><content type="html" xml:base="https://jgcarrasco.github.io/blog/2025/neural-physics-engine/"><![CDATA[<p>I recently found out about <a href="https://oasisaiminecraft.com/">Oasis</a>, an AI-generated Minecraft clone. You can actually play it right now from your browser, and I encourage you to do it! What you will see is entirely AI-generated: you select a world (which is just a screenshot of a scene) and then the following frames are entirely generated by a neural network.</p> <p>Roughly, my guess is they gathered lots of gameplay data and trained a model to predict the next frame given the current frame and user input. Once the model is trained, it can be used as some sort of “game engine” by just feeding a screenshot of the game. Then, the neural network will start generating the next frames conditioned on our input.</p> <p>I find this idea fascinating: we are replacing all the game/code logic by a neural network that has been only trained on image data.</p> <blockquote> <p>The idea of this blog post is to log my adventure into learning and (hopefully) building a similar AI-generated game engine. As an experiment, I wanted to make this public since the start so that I commit to it. However, expect this to be a little bit messy at first!</p> </blockquote> <h2 id="world-models">World Models</h2> <p>If we want to build a similar AI-generated game, the first thing to do is to look into the literature. I found out that Oasis is powered by a world model [1], whose task is to predict the next state of the world given the previous state/s and action/s. You can find quite a lot of papers related to world models, as well as technical info about Oasis [2].</p> <p>However, I think that it is important to not overcomplicate things when building something for the first time. In this specific case, it implies (i) avoiding models/techniques that require a huge amount of computing (e.g. training a ViT on lots of Minecraft gameplay) and (ii) avoiding super complicated techniques that provide marginal increases in performance. In other words, adding a complicated training scheme to improve performance by 2% may be essential when building an actual product, but if we want to learn, we should get away with the simplest solution.</p> <p>I think that IRIS [3] is a good candidate, as it is relatively simple, it works on small Atari games requiring small amounts of data and I think that it is also being used in production at comma.ai.</p> <p>Also note that most of the papers that talk about world models are related to Reinforcement Learning (RL). The main idea of all those papers is that RL agents are extremely sample inefficient, so it might be a good idea to learn a world model of the environment that can then be used to train the RL agent in its own “imagination”. Most papers follow a scheme of (i) gathering observations from the environment (ii) training the world model (iii) training the RL agent with the world model. In our case, we are only interested in the second step, while the RL agent is just used to gather data from the environment (instead of us having to play for hours to gather data). In other words, we actually don’t care that the RL agent performs well, we just want to have a decent world model.</p> <h2 id="iris">IRIS</h2> <p>Let’s look at the diagram below. We have the following components:</p> <ul> <li> <p>A <strong>discrete autoencoder</strong> composed by the encoder \(E\) and decoder \(D\). \(E\) is used to convert a frame into a set of \(K\) tokens \((z^1, z^2, ..., z^K)\), whereas \(D\) is used to reconstruct the original image from the tokens. Why is this required? Because we are going to use a transformer as the actual “engine”, and it only works at the level of tokens. Notice that we could treat each individual pixel as a single token, but as mentioned in the paper, the attention mechanism grows quadratically with the sequence length so this is unscalable.</p> </li> <li> <p>A <strong>transformer</strong> \(G\) to predict the next state of the environment. It receives sequences of consecutive frame/action tokens \((z_0^1,...,z_0^K,a_0,...,z_t^1, ...,z_t^{K})\)</p> </li> <li> <p>A <strong>policy</strong> \(\pi\), that will be used to select the action given the previous states. Obtaining a good policy is the main objective of the paper, but we actually care about properly modeling the game.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IRIS_diagram-480.webp 480w,/assets/img/IRIS_diagram-800.webp 800w,/assets/img/IRIS_diagram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IRIS_diagram.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>IRIS diagram. Source: [3]</p> <p>Once that world model is trained, we will feed the initial frame to the encoder to obtain the initial tokens \((z1_0,...,zK_0)\). Then, instead of sampling the action from the policy, we will retrieve it from the actual user, then use GPT to generate the tokens of the next state, and then decode them to obtain the next frame.</p> <p>That’s cool but, how do we actually train?. Essentially, each training step consists of three different processes:</p> <ol> <li><strong>Collect experience:</strong> Use the current policy to play the actual game and gather experience.</li> <li><strong>Update the world model:</strong> Use the previous experience to train E, D and G to properly predict the next observation, as well as the rewards and episode end.</li> <li><strong>Update the behavior:</strong> Improve the policy and value functions in the world model.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IRIS_algorithm-480.webp 480w,/assets/img/IRIS_algorithm-800.webp 800w,/assets/img/IRIS_algorithm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IRIS_algorithm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>IRIS algorithm. Source: [3]</p> <h2 id="getting-our-hands-dirty-ai-generated-pong">Getting our hands dirty: AI-Generated Pong</h2> <p>Instead of having to figure out everything before coding, let’s get our hands dirty by trying to get a minimal working implementation. My idea is to first take a simple game (such as Pong) and try training a world model with just a random policy. In other words, this means gathering data from games where the policy is just “move up/down/do nothing with 33% probability” and train the world model with that data. Then, we can think about actually including RL.</p> <ol> <li>Train a simple RL agent to collect experience in step 4. (Note that the aim of the paper is to train the agent in the imagination. But we are interested in collecting decent data. A random policy might not work because it will be very hard to return the ball, therefore the will not be able to feed data from a lot of situations to the world model.)</li> <li>Setup Pong</li> <li>Implement VQ-VAE</li> <li>Implement GPT</li> <li>Gather random experience and train it</li> <li>Visualize world model</li> <li>Interact with world model</li> </ol> <h3 id="references">References</h3> <p>[1] <a href="https://worldmodels.github.io/">https://worldmodels.github.io/</a></p> <p>[2] <a href="https://github.com/etched-ai/open-oasis/">https://github.com/etched-ai/open-oasis/</a></p> <p>[3] <a href="https://arxiv.org/pdf/2209.00588/">https://arxiv.org/pdf/2209.00588/</a></p> <h3 id="-under-construction-">=== UNDER CONSTRUCTION ===</h3>]]></content><author><name></name></author><category term="building"/><summary type="html"><![CDATA[Replacing game logic by a neural network]]></summary></entry></feed>