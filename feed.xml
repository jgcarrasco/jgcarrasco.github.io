<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jgcarrasco.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jgcarrasco.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-13T15:49:09+00:00</updated><id>https://jgcarrasco.github.io/feed.xml</id><title type="html">jgcarrasco</title><subtitle>Blog related to Deep Learning and Artificial Intelligence. </subtitle><entry><title type="html">Simulating games with neural networks</title><link href="https://jgcarrasco.github.io/blog/2025/neural-physics-engine/" rel="alternate" type="text/html" title="Simulating games with neural networks"/><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>https://jgcarrasco.github.io/blog/2025/neural-physics-engine</id><content type="html" xml:base="https://jgcarrasco.github.io/blog/2025/neural-physics-engine/"><![CDATA[<p>I recently found out about <a href="https://oasisaiminecraft.com/">Oasis</a>, an AI-generated Minecraft clone. You can actually play it right now from your browser, and I encourage you to do it! What you will see is entirely AI-generated: you select a world (which is just a screenshot of a scene) and then the following frames are entirely generated by a neural network.</p> <p>Roughly, my guess is they gathered lots of gameplay data and trained a model to predict the next frame given the current frame and user input. Once the model is trained, it can be used as some sort of “game engine” by just feeding a screenshot of the game. Then, the neural network will start generating the next frames conditioned on our input.</p> <p>I find this idea fascinating: we are replacing all the game/code logic by a neural network that has been only trained on image data.</p> <blockquote> <p>The idea of this blog post is to log my adventure into learning and (hopefully) building a similar AI-generated game engine. As an experiment, I wanted to make this public since the start so that I commit to it. However, expect this to be a little bit messy at first!</p> </blockquote> <h2 id="world-models">World Models</h2> <p>If we want to build a similar AI-generated game, the first thing to do is to look into the literature. I found out that Oasis is powered by a world model [1], whose task is to predict the next state of the world given the previous state/s and action/s. You can find quite a lot of papers related to world models, as well as technical info about Oasis [2].</p> <p>However, I think that it is important to not overcomplicate things when building something for the first time. In this specific case, it implies (i) avoiding models/techniques that require a huge amount of computing (e.g. training a ViT on lots of Minecraft gameplay) and (ii) avoiding super complicated techniques that provide marginal increases in performance. In other words, adding a complicated training scheme to improve performance by 2% may be essential when building an actual product, but if we want to learn, we should get away with the simplest solution.</p> <p>I think that IRIS [3] is a good candidate, as it is relatively simple, it works on small Atari games requiring small amounts of data and I think that it is also being used in production at comma.ai.</p> <p>Also note that most of the papers that talk about world models are related to Reinforcement Learning (RL). The main idea of all those papers is that RL agents are extremely sample inefficient, so it might be a good idea to learn a world model of the environment that can then be used to train the RL agent in its own “imagination”. Most papers follow a scheme of (i) gathering observations from the environment (ii) training the world model (iii) training the RL agent with the world model. In our case, we are only interested in the second step, while the RL agent is just used to gather data from the environment (instead of us having to play for hours to gather data). In other words, we actually don’t care that the RL agent performs well, we just want to have a decent world model.</p> <h2 id="iris">IRIS</h2> <p>Let’s look at the diagram below. We have the following components:</p> <ul> <li> <p>A <strong>discrete autoencoder</strong> composed by the encoder \(E\) and decoder \(D\). \(E\) is used to convert a frame into a set of \(K\) tokens \((z^1, z^2, ..., z^K)\), whereas \(D\) is used to reconstruct the original image from the tokens. Why is this required? Because we are going to use a transformer as the actual “engine”, and it only works at the level of tokens. Notice that we could treat each individual pixel as a single token, but as mentioned in the paper, the attention mechanism grows quadratically with the sequence length so this is unscalable.</p> </li> <li> <p>A <strong>transformer</strong> \(G\) to predict the next state of the environment. It receives sequences of consecutive frame/action tokens \((z_0^1,...,z_0^K,a_0,...,z_t^1, ...,z_t^{K})\)</p> </li> <li> <p>A <strong>policy</strong> \(\pi\), that will be used to select the action given the previous states. Obtaining a good policy is the main objective of the paper, but we actually care about properly modeling the game.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IRIS_diagram-480.webp 480w,/assets/img/IRIS_diagram-800.webp 800w,/assets/img/IRIS_diagram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IRIS_diagram.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>IRIS diagram. Source: [3]</p> <p>Once that world model is trained, we will feed the initial frame to the encoder to obtain the initial tokens \((z1_0,...,zK_0)\). Then, instead of sampling the action from the policy, we will retrieve it from the actual user, then use GPT to generate the tokens of the next state, and then decode them to obtain the next frame.</p> <p>That’s cool but, how do we actually train?. Essentially, each training step consists of three different processes:</p> <ol> <li><strong>Collect experience:</strong> Use the current policy to play the actual game and gather experience.</li> <li><strong>Update the world model:</strong> Use the previous experience to train \(E\), \(D\) and \(G\) to properly predict the next observation, as well as the rewards and episode end.</li> <li><strong>Update the behavior:</strong> Improve the policy and value functions in the world model.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IRIS_algorithm-480.webp 480w,/assets/img/IRIS_algorithm-800.webp 800w,/assets/img/IRIS_algorithm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IRIS_algorithm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>IRIS algorithm. Source: [3]</p> <h2 id="getting-our-hands-dirty-ai-generated-pong">Getting our hands dirty: AI-Generated Pong</h2> <p>Instead of having to figure out everything before coding, let’s get our hands dirty by trying to get a minimal working implementation. My idea is to first take a simple game (such as Pong) and try training a world model with just a random policy. In other words, this means gathering data from games where the policy is just “move up/down/do nothing with 33% probability” and train the world model with that data. Then, we can think about actually including RL.</p> <ol> <li>Setup Pong</li> <li>Implement VQ-VAE</li> <li>Implement GPT</li> <li>Gather random experience and train the world model</li> <li>Visualize world model</li> <li>Interact with world model</li> </ol> <h4 id="1-setting-up-pong">1. Setting up Pong</h4> <p>We are going to use <a href="https://gymnasium.farama.org/index.html">Gymnasium</a>, a maintained fork of OpenAI’s Gym RL library. This library is really convenient as it already has implemented all the common RL environments, including Pong and other Atari games. Setting up Pong is really easy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">ale_py</span>

<span class="n">gym</span><span class="p">.</span><span class="nf">register_envs</span><span class="p">(</span><span class="n">ale_py</span><span class="p">)</span>  <span class="c1"># optional, helpful for IDEs or pre-commit
</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">ALE/Pong-v5</span><span class="sh">"</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Reset the environment to start
</span><span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

<span class="n">frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">observation</span><span class="p">]</span>
<span class="c1"># Run a random action loop
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>  <span class="c1"># Choose a random action
</span>    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Take a step
</span>    <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>  <span class="c1"># Reset the environment if done or truncated
</span><span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>  <span class="c1"># Close the environment
</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoWriter</span><span class="p">(</span><span class="sh">'</span><span class="s">pong.mp4</span><span class="sh">'</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoWriter_fourcc</span><span class="p">(</span><span class="o">*</span><span class="sh">'</span><span class="s">H264</span><span class="sh">'</span><span class="p">),</span> <span class="mi">30</span><span class="p">,</span> <span class="p">(</span><span class="n">frames</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">frames</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
    <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>
<span class="n">out</span><span class="p">.</span><span class="nf">release</span><span class="p">()</span>
</code></pre></div></div> <p>The snippet above creates a Pong environment with <code class="language-plaintext highlighter-rouge">gym.make</code> and starts it with <code class="language-plaintext highlighter-rouge">env.reset()</code>. This function returns <code class="language-plaintext highlighter-rouge">observation</code>, which is a <code class="language-plaintext highlighter-rouge">(210x160, 3)</code> RGB array of the screen, and <code class="language-plaintext highlighter-rouge">info</code> about the environment. Then, for 1000 steps, it randomly selects an action and executes it inside of the game, obtaining the next <code class="language-plaintext highlighter-rouge">observation</code> of the environment, as well as the <code class="language-plaintext highlighter-rouge">reward</code> for taking the action, whether the agent reaches terminal state (<code class="language-plaintext highlighter-rouge">done</code>), and whether the execution has reached its limit (<code class="language-plaintext highlighter-rouge">truncated</code>).</p> <p>The last part of the snippet is just used to save a video of the simulation:</p> <figure> <video src="/assets/video/pong.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop=""/> </figure> <p>As you can see, our random strategy leaves a lot to be desired: it only returns the ball twice in 1000 steps, and ends up with a score of zero! This implies that the data collected from this random agent will leave out a lot of the game logic (e.g. the dynamics of the ball, the green score counter) that the world model will not be able to, well, model.</p> <p>In other words, we should expect that a world model trained from the random agent experience will properly model the movement of our player, and maybe the first collision of the ball, but the other mechanics will be undefined as it hasn’t seen any data about them.</p> <p>For now, we are going to implement the actual world model, train it with the experience of the random agent, and test whether this is true or not. Then, we will actually implement a proper RL agent.</p> <h3 id="-under-construction-">=== UNDER CONSTRUCTION ===</h3> <h4 id="2-implement-vq-vae">2. Implement VQ-VAE</h4> <p>For the implementation, I think that I am going to use the one from <a href="https://github.com/commaai/commavq/tree/master">commaVQ</a>.</p> <p>How does it work?</p> <p>Hyperparameters from IRIS paper</p> <p>Brief snippet about dimensions</p> <h3 id="references">References</h3> <p>[1] <a href="https://worldmodels.github.io/">https://worldmodels.github.io/</a></p> <p>[2] <a href="https://github.com/etched-ai/open-oasis/">https://github.com/etched-ai/open-oasis/</a></p> <p>[3] <a href="https://arxiv.org/pdf/2209.00588/">https://arxiv.org/pdf/2209.00588/</a></p>]]></content><author><name></name></author><category term="building"/><category term="log"/><summary type="html"><![CDATA[Replacing game logic by a neural network]]></summary></entry></feed>